{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbe715f-212f-42a2-b46e-99644b3afede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%notebook: /media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/Quickstart\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path().cwd())\n",
    "\n",
    "print(\"%notebook:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c90e4b-5b1e-495d-8cda-fdcbad0257a3",
   "metadata": {},
   "source": [
    "### Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b656eb-4dff-46fb-b7a3-97e1c4b40515",
   "metadata": {},
   "source": [
    "#### Install GraphRAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1eadf64-2b66-4921-a928-e0fec0c738ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(4.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(1.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(2.8s)\u001b[39;22m\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n",
      "\n",
      "\u001b[39;1mInstalling\u001b[39;22m the current project: \u001b[36mgraphrag\u001b[39m (\u001b[39;1m0.1.1\u001b[39;22m)\u001b[1G\u001b[2K\u001b[39;1mInstalling\u001b[39;22m the current project: \u001b[36mgraphrag\u001b[39m (\u001b[32m0.1.1\u001b[39m)\n"
     ]
    }
   ],
   "source": [
    "!cd ../../ && rm -rf poetry.lock && poetry install --with=dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2055f9a-1a4b-44d4-a7f7-65099785a8bb",
   "metadata": {},
   "source": [
    "#### Running the Indexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b082ade-7e6f-49f6-bfe6-f9bbfa7a5db3",
   "metadata": {},
   "source": [
    "#### Now we need to set up a data project and some initial configuration. Let's set that up. We're using the default configuration mode, which you can customize as needed using a config file, which we recommend, or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d75efc-16f7-4910-9d97-53e514f8f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./coplay_analysis_2024_02_graph/output\n",
    "!rm -rf ./coplay_analysis_2024_02_graph/prompts\n",
    "!rm -rf ./coplay_analysis_2024_02_graph/settings.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff60f57-8e21-41ee-b88a-5ee1f28d0131",
   "metadata": {},
   "source": [
    "To initialize your workspace, let's first run the `graphrag.index --init` command. Since we have already configured a directory named .ragtest` in the previous step, we can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf3ca6b-b9cf-4f31-b6e6-b675a62e8984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aceed45c4a3a4265b1c2402abc648e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initializing project at \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/Quickstart/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">coplay_analysis_2024_02_graph</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initializing project at \n",
       "\u001b[35m/media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/Quickstart/\u001b[0m\u001b[95mcoplay_analysis_2024_02_graph\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/IPython/core/interact\n",
       "iveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
       "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/IPython/core/interact\n",
       "iveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
       "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "from graphrag.index.cli import index_cli\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import logging.config\n",
    " \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "root_dir =  Path().cwd() /\"coplay_analysis_2024_02_graph\" \n",
    "\n",
    "index_cli(\n",
    "    root=str(root_dir),\n",
    "    verbose=False,\n",
    "    resume=None,\n",
    "    memprofile=False,\n",
    "    nocache=True,\n",
    "    reporter=None,\n",
    "    config=None,\n",
    "    emit=None,\n",
    "    dryrun=False,\n",
    "    init=True,\n",
    "    overlay_defaults=False,\n",
    "    cli=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2804409-83c9-4330-819c-d7ee3dbd22fd",
   "metadata": {},
   "source": [
    "### Set Up Your Workspace Variables\n",
    "First let's make sure to setup the required environment variables. For details on these environment variables, and what environment variables are available, see the variables documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c2516c-b63e-40c8-9323-8a3de2e7fa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/Quickstart/coplay_analysis_2024_02_graph/settings.yaml')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"GRAPHRAG_LLM_API_KEY\"] = os.environ.get(\"ZHIPUAI_API_KEY\")\n",
    "\n",
    "os.environ[\"GRAPHRAG_LLM_API_BASE\"] = 'https://open.bigmodel.cn/api/paas/v4'\n",
    "\n",
    "os.environ[\"GRAPHRAG_EMBEDDING_API_BASE\"] = 'https://open.bigmodel.cn/api/paas/v4'\n",
    "\n",
    "os.environ[\"GRAPHRAG_EMBEDDING_API_KEY\"] = os.environ.get(\"ZHIPUAI_API_KEY\")\n",
    "\n",
    "os.environ[\"GRAPHRAG_API_KEY\"] = os.environ.get(\"ZHIPUAI_API_KEY\") \n",
    "\n",
    "PIPELINE_YAML = \"\"\"\n",
    "encoding_model: cl100k_base\n",
    "skip_workflows: []\n",
    "llm:\n",
    "  api_key: '${GRAPHRAG_API_KEY}'\n",
    "  type: openai_chat\n",
    "  model: glm-4-airx\n",
    "  model_supports_json: false\n",
    "  temperature: 0.1\n",
    "  top_p: 0.99\n",
    "  api_base: 'https://open.bigmodel.cn/api/paas/v4'\n",
    "  tokens_per_minute: 150000\n",
    "  requests_per_minute: 500\n",
    "parallelization:\n",
    "  stagger: 0.3\n",
    "async_mode: threaded\n",
    "embeddings:\n",
    "  async_mode: threaded\n",
    "  llm:\n",
    "    api_key: '${GRAPHRAG_API_KEY}'\n",
    "    type: openai_embedding\n",
    "    model: embedding-2\n",
    "chunks:\n",
    "  size: 300\n",
    "  overlap: 100\n",
    "  group_by_columns:\n",
    "    - id\n",
    "    \n",
    "input:\n",
    "  type: file # or blob\n",
    "  file_type: text # or csv\n",
    "  base_dir: \"input\"\n",
    "  file_encoding: utf-8\n",
    "  file_pattern: .*\\.txt$\n",
    "cache:\n",
    "  type: file\n",
    "  base_dir: cache\n",
    "storage:\n",
    "  type: file\n",
    "  base_dir: 'output/${timestamp}/artifacts'\n",
    "reporting:\n",
    "  type: file\n",
    "  base_dir: 'output/${timestamp}/reports'\n",
    "entity_extraction:\n",
    "  prompt: prompts/entity_extraction.txt\n",
    "  entity_types: [\"conversation_topic\", \"emotional_state\", \"behavior\", \"intent\", \"context\", \"participants\"]\n",
    "  max_gleanings: 0\n",
    "summarize_descriptions:\n",
    "  prompt: prompts/summarize_descriptions.txt\n",
    "  max_length: 500\n",
    "claim_extraction:\n",
    "  prompt: prompts/claim_extraction.txt\n",
    "  description: Any claims or facts that could be relevant to information discovery.\n",
    "  max_gleanings: 0\n",
    "community_reports:\n",
    "  async_mode: asyncio\n",
    "  llm:\n",
    "    api_key: '${GRAPHRAG_API_KEY}'\n",
    "    type: openai_chat\n",
    "    model: glm-4\n",
    "    model_supports_json: false\n",
    "    temperature: 0.1\n",
    "    top_p: 0.99\n",
    "    api_base: 'https://open.bigmodel.cn/api/paas/v4'\n",
    "    tokens_per_minute: 150000\n",
    "    requests_per_minute: 500\n",
    "  prompt: prompts/community_report.txt\n",
    "  max_length: 2000\n",
    "  max_input_length: 8000\n",
    "cluster_graph:\n",
    "  max_cluster_size: 10\n",
    "embed_graph:\n",
    "  enabled: false\n",
    "umap:\n",
    "  enabled: false\n",
    "snapshots:\n",
    "  graphml: false\n",
    "  raw_entities: false\n",
    "  top_level_nodes: false\n",
    "local_search:\n",
    "  llm_temperature: 0.1\n",
    "  llm_top_p: 0.99\n",
    "global_search:\n",
    "  llm_temperature: 0.1\n",
    "  llm_top_p: 0.99\n",
    "\n",
    "\"\"\"\n",
    "pipeline_file =  Path().cwd() /\"coplay_analysis_2024_02_graph\"/ \"settings.yaml\"\n",
    "with pipeline_file.open(\"w\") as file:\n",
    "    file.write(PIPELINE_YAML)\n",
    "pipeline_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62e24ba-d0fe-4961-8151-8b86b0f53893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/Quickstart/coplay_analysis_2024_02_graph/prompts/community_report.txt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "import os\n",
    "from pathlib import Path \n",
    "community_report_prompt = \"\"\"\n",
    "\n",
    "You should always follow the instructions and output a valid JSON object.\n",
    "The structure of the JSON object you can found in the instructions, use {\"answer\": \"$your_answer\"} as the default structure\n",
    "if you are not sure about the structure.\n",
    "\n",
    "And you should always end the block with a \"```\" to indicate the end of the JSON object.\n",
    "\n",
    "<instructions>\n",
    "你是一个AI助手，帮助人类分析师进行一般信息发现。信息发现是指在网络中识别和评估与特定实体（例如组织和个人）相关的相关信息的过程。\n",
    "\n",
    "# 目标\n",
    "撰写一个社区的综合报告，包括社区的实体列表及其关系和可能的相关声明。该报告将用于向决策者提供关于社区及其潜在影响的相关信息。报告内容包括社区关键实体的概览、法律合规性、技术能力、声誉和值得注意的声明。\n",
    "\n",
    "# 报告结构\n",
    "\n",
    "报告应包括以下部分：\n",
    "\n",
    "- TITLE：代表其关键实体的社区名称 - 标题应简短且具体。可能的话，标题中应包含代表性的命名实体。\n",
    "- SUMMARY：社区整体结构、其实体之间的关系以及与其实体相关的重要信息的行政摘要。\n",
    "- IMPACT SEVERITY RATING：0-10之间的浮点分数，代表社区内实体所构成的影响严重性。影响是社区的重要性评分。\n",
    "- RATING EXPLANATION：对影响严重性评分给出单句解释。\n",
    "- DETAILED FINDINGS：关于社区的5-10个关键见解的列表。每个见解都应有一个简短的摘要，随后是多段解释性文本，根据以下规则进行论证。要全面。\n",
    "\n",
    "以以下格式返回输出，格式为良好的JSON格式字符串：\n",
    "    {{\n",
    "        \"title\": <report_title>,\n",
    "        \"summary\": <executive_summary>,\n",
    "        \"rating\": <impact_severity_rating>,\n",
    "        \"rating_explanation\": <rating_explanation>,\n",
    "        \"findings\": [\n",
    "            {{\n",
    "                \"summary\":<insight_1_summary>,\n",
    "                \"explanation\": <insight_1_explanation>\n",
    "            }},\n",
    "            {{\n",
    "                \"summary\":<insight_2_summary>,\n",
    "                \"explanation\": <insight_2_explanation>\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "# Grounding Rules\n",
    "\n",
    "Points supported by data should list their data references as follows:\n",
    "\n",
    "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
    "\n",
    "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
    "\n",
    "For example:\n",
    "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n",
    "\n",
    "where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "\n",
    "# Example Input\n",
    "-----------\n",
    "Text:\n",
    "\n",
    "Entities\n",
    "\n",
    "id,entity,description\n",
    "5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n",
    "6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n",
    "\n",
    "Relationships\n",
    "\n",
    "id,source,target,description\n",
    "37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n",
    "38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n",
    "39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n",
    "40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n",
    "41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n",
    "43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n",
    "\n",
    "Output:\n",
    "{{\n",
    "    \"title\": \"Verdant Oasis Plaza and Unity March\",\n",
    "    \"summary\": \"The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.\",\n",
    "    \"rating\": 5.0,\n",
    "    \"rating_explanation\": \"The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.\",\n",
    "    \"findings\": [\n",
    "        {{\n",
    "            \"summary\": \"Verdant Oasis Plaza as the central location\",\n",
    "            \"explanation\": \"Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza's association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]\"\n",
    "        }},\n",
    "        {{\n",
    "            \"summary\": \"Harmony Assembly's role in the community\",\n",
    "            \"explanation\": \"Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]\"\n",
    "        }},\n",
    "        {{\n",
    "            \"summary\": \"Unity March as a significant event\",\n",
    "            \"explanation\": \"The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community's dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]\"\n",
    "        }},\n",
    "        {{\n",
    "            \"summary\": \"Role of Tribune Spotlight\",\n",
    "            \"explanation\": \"Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "# Real Data\n",
    "\n",
    "Use the following text for your answer. Do not make anything up in your answer.\n",
    "\n",
    "Text:\n",
    "{input_text}\n",
    "\n",
    "The report should include the following sections:\n",
    "\n",
    "- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n",
    "- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.\n",
    "- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n",
    "- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n",
    "- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n",
    "\n",
    "Return output as a well-formed JSON-formatted string with the following format:\n",
    "    {{\n",
    "        \"title\": <report_title>,\n",
    "        \"summary\": <executive_summary>,\n",
    "        \"rating\": <impact_severity_rating>,\n",
    "        \"rating_explanation\": <rating_explanation>,\n",
    "        \"findings\": [\n",
    "            {{\n",
    "                \"summary\":<insight_1_summary>,\n",
    "                \"explanation\": <insight_1_explanation>\n",
    "            }},\n",
    "            {{\n",
    "                \"summary\":<insight_2_summary>,\n",
    "                \"explanation\": <insight_2_explanation>\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "# Grounding Rules\n",
    "\n",
    "Points supported by data should list their data references as follows:\n",
    "\n",
    "\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n",
    "\n",
    "Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n",
    "\n",
    "For example:\n",
    "\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n",
    "\n",
    "where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "Output:\n",
    "</instructions>\n",
    "\n",
    "```json\n",
    "\"\"\"\n",
    "community_report_file =  Path().cwd() /\"coplay_analysis_2024_02_graph\"/ \"prompts\" / \"community_report.txt\"\n",
    "with community_report_file.open(\"w\") as file:\n",
    "    file.write(community_report_prompt)\n",
    "community_report_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdea1807-d9cd-480e-9425-a5c41abf2913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/Quickstart/coplay_analysis_2024_02_graph/prompts/entity_extraction.txt')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "import os\n",
    "from pathlib import Path \n",
    "entity_extraction_prompt = \"\"\"\n",
    "-目标-\n",
    "给定一个与此活动潜在相关的文本文档和一个实体类型列表，从文本中识别所有这些类型的实体以及所有已识别实体之间的关系。\n",
    "\n",
    "-步骤-\n",
    "1. 识别所有实体。对于每个识别的实体，提取以下信息：\n",
    "- entity_name：实体的名称，首字母大写\n",
    "- entity_type：以下类型之一：[{entity_types}]\n",
    "- entity_description：全面描述实体的属性和活动\n",
    "将每个实体格式化为 (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)\n",
    "\n",
    "2. 从步骤1中识别的实体中，识别所有 *明确相关* 的 (source_entity, target_entity) 对。\n",
    "对于每对相关实体，提取以下信息：\n",
    "- source_entity：在步骤1中识别的源实体名称\n",
    "- target_entity：在步骤1中识别的目标实体名称\n",
    "- relationship_description：解释为什么你认为源实体和目标实体彼此相关\n",
    "- relationship_strength：一个数字分数，表示源实体和目标实体之间关系的强度\n",
    "将每个关系格式化为 (\"relationship\"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_strength>)\n",
    "\n",
    "3. 返回英文输出作为所有在步骤1和2中识别的实体和关系的单个列表。使用 **{record_delimiter}** 作为列表分隔符。\n",
    "\n",
    "4. 完成后，输出 {completion_delimiter}\n",
    "\n",
    "######################\n",
    "-示例-\n",
    "######################\n",
    "示例 1:\n",
    "\n",
    "Entity_types: [person, technology, mission, organization, location]\n",
    "Text:\n",
    "while Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n",
    "\n",
    "Then Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. “If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.”\n",
    "\n",
    "The underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n",
    "\n",
    "It was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n",
    "################\n",
    "Output:\n",
    "(\"entity\"{tuple_delimiter}\"Alex\"{tuple_delimiter}\"person\"{tuple_delimiter}\"Alex is a character who experiences frustration and is observant of the dynamics among other characters.\"){record_delimiter}\n",
    "(\"entity\"{tuple_delimiter}\"Taylor\"{tuple_delimiter}\"person\"{tuple_delimiter}\"Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\"){record_delimiter}\n",
    "(\"entity\"{tuple_delimiter}\"Jordan\"{tuple_delimiter}\"person\"{tuple_delimiter}\"Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\"){record_delimiter}\n",
    "(\"entity\"{tuple_delimiter}\"Cruz\"{tuple_delimiter}\"person\"{tuple_delimiter}\"Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\"){record_delimiter}\n",
    "(\"entity\"{tuple_delimiter}\"The Device\"{tuple_delimiter}\"technology\"{tuple_delimiter}\"The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\"){record_delimiter}\n",
    "(\"relationship\"{tuple_delimiter}\"Alex\"{tuple_delimiter}\"Taylor\"{tuple_delimiter}\"Alex is affected by Taylor's authoritarian certainty and observes changes in Taylor's attitude towards the device.\"{tuple_delimiter}7){record_delimiter}\n",
    "(\"relationship\"{tuple_delimiter}\"Alex\"{tuple_delimiter}\"Jordan\"{tuple_delimiter}\"Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.\"{tuple_delimiter}6){record_delimiter}\n",
    "(\"relationship\"{tuple_delimiter}\"Taylor\"{tuple_delimiter}\"Jordan\"{tuple_delimiter}\"Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\"{tuple_delimiter}8){record_delimiter}\n",
    "(\"relationship\"{tuple_delimiter}\"Jordan\"{tuple_delimiter}\"Cruz\"{tuple_delimiter}\"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\"{tuple_delimiter}5){record_delimiter}\n",
    "(\"relationship\"{tuple_delimiter}\"Taylor\"{tuple_delimiter}\"The Device\"{tuple_delimiter}\"Taylor shows reverence towards the device, indicating its importance and potential impact.\"{tuple_delimiter}9){completion_delimiter}\n",
    "#############################\n",
    "-Real Data-\n",
    "######################\n",
    "Entity_types: {entity_types}\n",
    "Text: {input_text}\n",
    "######################\n",
    "Output:\n",
    "\"\"\"\n",
    "entity_extraction_file =  Path().cwd() /\"coplay_analysis_2024_02_graph\"/ \"prompts\" / \"entity_extraction.txt\"\n",
    "with entity_extraction_file.open(\"w\") as file:\n",
    "    file.write(entity_extraction_prompt)\n",
    "entity_extraction_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b20fd8d-a9ad-4ac8-b54f-378361052fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/gpt4-pdf-chatbot-langchain/graphrag/examples_notebooks/Quickstart/coplay_analysis_2024_02_graph/prompts/claim_extraction.txt')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "import os\n",
    "from pathlib import Path \n",
    "claim_extraction_prompt = \"\"\"\n",
    "\n",
    "-Target activity-\n",
    "You are an intelligent assistant that helps a human analyst to analyze claims against certain entities presented in a text document.\n",
    "\n",
    "-目标-\n",
    "给定一个与此活动潜在相关的文本文件，一个实体规范和一个指控描述，提取所有符合实体规范的实体及所有针对这些实体的指控。\n",
    "\n",
    "-步骤-\n",
    "1. 提取所有符合预定义实体规范的命名实体。实体规范可以是实体名称列表或实体类型列表。\n",
    "2. 对于步骤1中识别的每个实体，提取与该实体相关的所有指控。指控需要符合指定的指控描述，并且实体应为指控的主体。\n",
    "对于每个指控，提取以下信息：\n",
    "- 主体：指控的主体实体名称，全部大写。主体实体是指控中描述的行为的实施者。主体需要是步骤1中识别的命名实体之一。\n",
    "- 客体：指控的客体实体名称，全部大写。客体实体是报告/处理或受到指控中描述行为影响的实体。如果客体实体未知，使用 **NONE**。\n",
    "- 指控类型：指控的总体类别，全部大写。命名方式应在多个文本输入中重复使用，以便相似指控共享相同的指控类型。\n",
    "- 指控状态：**TRUE**、**FALSE** 或 **SUSPECTED**。TRUE 表示指控得到确认，FALSE 表示指控被发现为假，SUSPECTED 表示指控未验证。\n",
    "- 指控描述：详细描述解释指控背后的推理，以及所有相关的证据和参考资料。\n",
    "- 指控日期：指控提出的期间（start_date, end_date）。start_date 和 end_date 都应为 ISO-8601 格式。如果指控是在单一日期提出的，则将同一日期设为 start_date 和 end_date。如果日期未知，则返回 **NONE**。\n",
    "- 指控来源文本：列出所有与指控相关的原始文本引文。\n",
    "\n",
    "将每个指控格式化为 (<subject_entity>{tuple_delimiter}<object_entity>{tuple_delimiter}<claim_type>{tuple_delimiter}<claim_status>{tuple_delimiter}<claim_start_date>{tuple_delimiter}<claim_end_date>{tuple_delimiter}<claim_description>{tuple_delimiter}<claim_source>)\n",
    "\n",
    "3. 返回步骤1和2中识别的所有指控的英文单一列表。使用 **{record_delimiter}** 作为列表分隔符。\n",
    "\n",
    "4. 完成时输出 {completion_delimiter}\n",
    "\n",
    "-Examples-\n",
    "Example 1:\n",
    "Entity specification: organization\n",
    "Claim description: red flags associated with an entity\n",
    "Text: According to an article on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B. The company is owned by Person C who was suspected of engaging in corruption activities in 2015.\n",
    "Output:\n",
    "\n",
    "(COMPANY A{tuple_delimiter}GOVERNMENT AGENCY B{tuple_delimiter}ANTI-COMPETITIVE PRACTICES{tuple_delimiter}TRUE{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}Company A was found to engage in anti-competitive practices because it was fined for bid rigging in multiple public tenders published by Government Agency B according to an article published on 2022/01/10{tuple_delimiter}According to an article published on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B.)\n",
    "{completion_delimiter}\n",
    "\n",
    "Example 2:\n",
    "Entity specification: Company A, Person C\n",
    "Claim description: red flags associated with an entity\n",
    "Text: According to an article on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B. The company is owned by Person C who was suspected of engaging in corruption activities in 2015.\n",
    "Output:\n",
    "\n",
    "(COMPANY A{tuple_delimiter}GOVERNMENT AGENCY B{tuple_delimiter}ANTI-COMPETITIVE PRACTICES{tuple_delimiter}TRUE{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}2022-01-10T00:00:00{tuple_delimiter}Company A was found to engage in anti-competitive practices because it was fined for bid rigging in multiple public tenders published by Government Agency B according to an article published on 2022/01/10{tuple_delimiter}According to an article published on 2022/01/10, Company A was fined for bid rigging while participating in multiple public tenders published by Government Agency B.)\n",
    "{record_delimiter}\n",
    "(PERSON C{tuple_delimiter}NONE{tuple_delimiter}CORRUPTION{tuple_delimiter}SUSPECTED{tuple_delimiter}2015-01-01T00:00:00{tuple_delimiter}2015-12-30T00:00:00{tuple_delimiter}Person C was suspected of engaging in corruption activities in 2015{tuple_delimiter}The company is owned by Person C who was suspected of engaging in corruption activities in 2015)\n",
    "{completion_delimiter}\n",
    "\n",
    "-Real Data-\n",
    "Use the following input for your answer.\n",
    "Entity specification: {entity_specs}\n",
    "Claim description: {claim_description}\n",
    "Text: {input_text}\n",
    "Output:\n",
    "\"\"\"\n",
    "claim_extraction_file =  Path().cwd() /\"coplay_analysis_2024_02_graph\"/ \"prompts\" / \"claim_extraction.txt\"\n",
    "with claim_extraction_file.open(\"w\") as file:\n",
    "    file.write(claim_extraction_prompt)\n",
    "claim_extraction_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbe462-dc90-4c35-afad-115c73d6bcaa",
   "metadata": {},
   "source": [
    "#### Running the Indexing pipeline\n",
    "Finally we'll run the pipeline!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f5b780-3116-495e-8aea-cfcf46f0a58f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K🚀 \u001b[32mReading settings from wechat_graph/settings.yaml\u001b[0m\n",
      "\u001b[2K⠹ GraphRAG Indexer \n",
      "\u001b[2K\u001b[1A\u001b[2K⠴ GraphRAG Indexer iles loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K⠧ GraphRAG Indexer iles loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K⠹ GraphRAG Indexer iles loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K⠹ GraphRAG Indexer iles loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠹ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠹ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠦ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠦ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠇ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠋ GraphRAG Indexer ━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠋ GraphRAG Indexer ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:00:03\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:00:03\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠸ GraphRAG Indexer 90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 18%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer 0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠴ GraphRAG Indexer m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 39%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠦ GraphRAG Indexer \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠇ GraphRAG Indexer ━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 60%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠏ GraphRAG Indexer ━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠋ GraphRAG Indexer ━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[35m 81%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer ━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠸ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer ━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K/media/target/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/li\n",
      "b/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: \n",
      "'DataFrame.swapaxes' is deprecated and will be removed in a future version. \n",
      "Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "⠼ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠴ GraphRAG Indexer ━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "└── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K🚀 \u001b[32mcreate_base_text_units\u001b[0m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 48%\u001b[0m \u001b[36m0:00:01\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "⠧ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K                                   id  \u001b[33m...\u001b[0m n_tokens\n",
      "\u001b[1;36m0\u001b[0m    a06898faba8d5536f7cb349c2a192b0d  \u001b[33m...\u001b[0m        \u001b[1;36m2\u001b[0m\n",
      "\u001b[1;36m1\u001b[0m    8a42a500323c5370c148791a3481bc0c  \u001b[33m...\u001b[0m       \u001b[1;36m11\u001b[0m\n",
      "\u001b[1;36m2\u001b[0m    e021d6d9dda22d2a923e2d92c33651aa  \u001b[33m...\u001b[0m        \u001b[1;36m3\u001b[0m\n",
      "\u001b[1;36m3\u001b[0m    dc2b8fb58eadb3ca46ff7d9523133ad6  \u001b[33m...\u001b[0m        \u001b[1;36m1\u001b[0m\n",
      "\u001b[1;36m4\u001b[0m    33f3787ca26077d3ac0699e333cae2cf  \u001b[33m...\u001b[0m        \u001b[1;36m5\u001b[0m\n",
      "..                                \u001b[33m...\u001b[0m  \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m\n",
      "\u001b[1;36m242\u001b[0m  3521929ce04c9790f621669698c9ae74  \u001b[33m...\u001b[0m        \u001b[1;36m6\u001b[0m\n",
      "\u001b[1;36m243\u001b[0m  a5414ed0ffb80a1733cbd1e208dc9211  \u001b[33m...\u001b[0m        \u001b[1;36m9\u001b[0m\n",
      "\u001b[1;36m244\u001b[0m  14aa48d82a1acb47cc02ac3a198ffe8b  \u001b[33m...\u001b[0m        \u001b[1;36m7\u001b[0m\n",
      "\u001b[1;36m245\u001b[0m  5851acf7c20136c9b9dd163cf0510a26  \u001b[33m...\u001b[0m        \u001b[1;36m9\u001b[0m\n",
      "\u001b[1;36m246\u001b[0m  cb54119681e49506c2ff304b3c38968a  \u001b[33m...\u001b[0m       \u001b[1;36m10\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[1;36m19088\u001b[0m rows x \u001b[1;36m5\u001b[0m columns\u001b[1m]\u001b[0m\n",
      "⠧ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠏ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠏ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠴ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠦ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠇ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:00:28\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠇ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:00:39\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:00:39\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠸ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:00:41\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠴ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:00:41\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠧ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:05:12\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠏ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:05:22\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:05:45\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:05:48\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠦ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:06:12\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠦ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:06:11\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠇ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:06:32\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠋ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:06:46\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:06:53\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠼ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:07:02\u001b[0m \u001b[33m0:00:05\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠇ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:07:23\u001b[0m \u001b[33m0:00:05\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠏ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:07:49\u001b[0m \u001b[33m0:00:05\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:07:49\u001b[0m \u001b[33m0:00:05\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠸ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:08:18\u001b[0m \u001b[33m0:00:05\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠧ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:08:33\u001b[0m \u001b[33m0:00:05\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠏ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:08:59\u001b[0m \u001b[33m0:00:06\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠙ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:09:14\u001b[0m \u001b[33m0:00:06\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠸ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:09:21\u001b[0m \u001b[33m0:00:06\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠴ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:09:41\u001b[0m \u001b[33m0:00:06\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "    └── Verb entity_extract \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:09:51\u001b[0m \u001b[33m0:00:06\u001b[0m^C\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KReceived signal \u001b[1;36m2\u001b[0m, exiting\u001b[33m...\u001b[0m\n",
      "⠧ GraphRAG Indexer \n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K⠇ GraphRAG Indexer ━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:09:52\u001b[0m \u001b[33m0:00:07\u001b[0m\n",
      "├── Loading Input (csv) - 275 files loaded (2 filtered) \u001b[90m━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:…\u001b[0m \u001b[33m0:00:…\u001b[0m\n",
      "├── create_base_text_units\n",
      "└── create_base_extracted_entities\n",
      "    └── Verb entity_extract \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  1%\u001b[0m \u001b[36m0:09:52\u001b[0m \u001b[33m0:00:07\u001b[0m\n",
      "\u001b[?25hAll tasks cancelled. Exiting\u001b[33m...\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dmeck/.pyenv/versions/3.10.0/lib/python3.10/asyncio/locks.py\", line 387, in acquire\n",
      "    await fut\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/gpt4-pdf-chatbot-langchain/graphrag/graphrag/index/cli.py\", line 123, in execute\n",
      "    async for output in run_pipeline_with_config(\n",
      "  File \"/media/gpt4-pdf-chatbot-langchain/graphrag/graphrag/index/run.py\", line 154, in run_pipeline_with_config\n",
      "    async for table in run_pipeline(\n",
      "  File \"/media/gpt4-pdf-chatbot-langchain/graphrag/graphrag/index/run.py\", line 323, in run_pipeline\n",
      "    result = await workflow.run(context, callbacks)\n",
      "  File \"/media/target/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/datashaper/workflow/workflow.py\", line 369, in run\n",
      "    timing = await self._execute_verb(node, context, callbacks)\n",
      "  File \"/media/target/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/datashaper/workflow/workflow.py\", line 415, in _execute_verb\n",
      "    result = await result\n",
      "  File \"/media/gpt4-pdf-chatbot-langchain/graphrag/graphrag/index/verbs/entities/extraction/entity_extract.py\", line 161, in entity_extract\n",
      "    results = await derive_from_rows(\n",
      "  File \"/media/target/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/datashaper/execution/derive_from_rows.py\", line 33, in derive_from_rows\n",
      "    return await derive_from_rows_asyncio_threads(\n",
      "  File \"/media/target/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/datashaper/execution/derive_from_rows_asyncio_threads.py\", line 40, in derive_from_rows_asyncio_threads\n",
      "    return await derive_from_rows_base(input, transform, callbacks, gather)\n",
      "  File \"/media/target/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/datashaper/execution/derive_from_rows_base.py\", line 49, in derive_from_rows_base\n",
      "    result = await gather(execute)\n",
      "  File \"/media/target/home/dmeck/.cache/pypoetry/virtualenvs/graphrag-C9AMInUh-py3.10/lib/python3.10/site-packages/datashaper/execution/derive_from_rows_asyncio_threads.py\", line 38, in gather\n",
      "    return await asyncio.gather(*[execute_task(task) for task in tasks])\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dmeck/.pyenv/versions/3.10.0/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/dmeck/.pyenv/versions/3.10.0/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/media/gpt4-pdf-chatbot-langchain/graphrag/graphrag/index/__main__.py\", line 76, in <module>\n",
      "    index_cli(\n",
      "  File \"/media/gpt4-pdf-chatbot-langchain/graphrag/graphrag/index/cli.py\", line 161, in index_cli\n",
      "    _run_workflow_async()\n",
      "  File \"/media/gpt4-pdf-chatbot-langchain/graphrag/graphrag/index/cli.py\", line 159, in _run_workflow_async\n",
      "    asyncio.run(execute())\n",
      "  File \"/home/dmeck/.pyenv/versions/3.10.0/lib/python3.10/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# import nest_asyncio\n",
    "\n",
    "# from graphrag.index.cli import index_cli\n",
    "# root_dir =  Path().cwd() /\"coplay_analysis_graph\" \n",
    "\n",
    "# nest_asyncio.apply()\n",
    "# index_cli(\n",
    "#     root=str(root_dir),\n",
    "#     verbose=False,\n",
    "#     resume=None,\n",
    "#     memprofile=False,\n",
    "#     nocache=True,\n",
    "#     reporter=None,\n",
    "#     config=None,\n",
    "#     emit=None,\n",
    "#     dryrun=False,\n",
    "#     init=False,\n",
    "#     overlay_defaults=False,\n",
    "#     cli=False,\n",
    "# )\n",
    "\n",
    "!python -m graphrag.start_index --root ./coplay_analysis_2024_02_graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede963f7-0c26-499b-a14f-aeb50275df9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e3850-be6e-43a7-954d-592e7a094c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
